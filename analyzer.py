import sys
import os
import subprocess
import re
import logging
import io
import tempfile
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv
load_dotenv()



# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –£–°–¢–ê–ù–û–í–ö–ê ---
logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–º–µ–Ω–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ (pip) –∏ –∏–º–µ–Ω–∏ –∏–º–ø–æ—Ä—Ç–∞ (import)
PACKAGE_MAPPING: List[Tuple[str, str]] = [
    ("speech_recognition", "speech_recognition"),
    ("pydub", "pydub"),
    ("rapidfuzz", "rapidfuzz"),
    ("torch", "torch"),
    ("transformers", "transformers"),
    ("python-telegram-bot", "telegram"), 
]

def install_and_import(package_list: List[Tuple[str, str]]):
    logging.info("–ü—Ä–æ–≤–µ—Ä—è—é –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏...")
    for pip_name, import_name in package_list:
        try:
            __import__(import_name)
        except ImportError:
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", "-U", "--no-cache-dir", "-q", pip_name])
                logging.info(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {pip_name}")
            except subprocess.CalledProcessError as e:
                logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å {pip_name}: {e}")
                sys.exit(1)
        try:
             __import__(import_name)
        except Exception as e:
             logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å {import_name} –ø–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
             sys.exit(1)

# –í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ —Å –Ω–æ–≤—ã–º —Å–ø–∏—Å–∫–æ–º
install_and_import(PACKAGE_MAPPING)

import speech_recognition as sr
from pydub import AudioSegment
from rapidfuzz import fuzz
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from html import escape # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è HTML-—Å–∏–º–≤–æ–ª–æ–≤

# --- –ù–ê–°–¢–†–û–ô–ö–ê AI-–ú–û–î–ï–õ–ò –ò –¢–û–ö–ï–ù–ê ---
# !!! –ó–ê–ú–ï–ù–ò–¢–ï –≠–¢–û–¢ –¢–û–ö–ï–ù –ù–ê –í–ê–® –ê–ö–¢–£–ê–õ–¨–ù–´–ô !!!
BOT_TOKEN = os.getenv("BOT_TOKEN") or "–í–ê–®_–¢–û–ö–ï–ù_–ë–û–¢–ê_–ó–î–ï–°–¨"

MODEL_NAME = "cointegrated/rubert-tiny2-cedr-emotion-detection" 
tokenizer = None
model = None


def load_ai_model():
    global tokenizer, model
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
        logging.info("AI-–º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
    except Exception as e:
        logging.error(f"–ù–ï –£–î–ê–õ–û–°–¨ –ó–ê–ì–†–£–ó–ò–¢–¨ AI-–ú–û–î–ï–õ–¨ (NLP). AI-–ê–ù–ê–õ–ò–ó –ë–£–î–ï–¢ –ù–ï–î–û–°–¢–£–ü–ï–ù. –ü—Ä–∏—á–∏–Ω–∞: {e}")
        tokenizer = None
        model = None

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –±–æ—Ç–∞
load_ai_model()


# --- –§–†–ê–ó–´ –ò –ü–ê–¢–¢–ï–†–ù–´ ---
FUZZY_THRESHOLD = 70
FRAUD_KEYPHRASES = [
    "–≤—ã –≤—ã–∏–≥—Ä–∞–ª–∏", "–ø–µ—Ä–µ–≤–µ–¥–∏—Ç–µ –¥–µ–Ω—å–≥–∏", "–Ω–æ–º–µ—Ä –∫–∞—Ä—Ç—ã", "–∫–æ–¥ –∏–∑ —Å–º—Å",
    "cvv", "–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –ø–ª–∞—Ç–µ–∂", "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è", "–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–∏–±—ã–ª—å",
    "–±–µ–∑ —Ä–∏—Å–∫–∞", "–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ", "–∫–æ–º–∏—Å—Å–∏—è", "—Å–∫–∞—á–∞–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ",
    "–≤–∞—à —Å—á–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω", "—Å–ª—É–∂–±–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –±–∞–Ω–∫–∞", "—Å—Ä–æ–∫ –¥–µ–π—Å—Ç–≤–∏—è –∫–∞—Ä—Ç—ã",
    "click the link", "transfer money", "verify your account", "–ø–æ—Ç–µ—Ä—è–µ—Ç–µ –¥–µ–Ω—å–≥–∏",
    "–ª—è–º", "–º–∏–ª–ª–∏–æ–Ω", "–∫—Ä—É–ø–Ω–∞—è —Å—É–º–º–∞", "–∫–æ–¥ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è",
    "—Å–∫–∞–∂–∏—Ç–µ –∫–æ–¥", "–æ—Ç–ø—Ä–∞–≤—å—Ç–µ –∫–æ–¥", "—Å–∫–∞–∂–∏—Ç–µ —Ü–∏—Ñ—Ä—ã"
]
REGEX_PATTERNS = {
    "card_number": r"\b\d{4}[ -]?\d{4}[ -]?\d{4}[ -]?\d{4}\b|\b(?:\d[ -]*?){13,19}\b",
    "phone": r"\b(?:7|8)[-\s]?\d{3}[-\s]?\d{3}[-\s]?\d{2}[-\s]?\d{2}\b|\b(\+?\d{1,3}[-\s]?)?(\(?\d{3,4}\)?[-\s]?)?\d{3,4}[-\s]?\d{2,4}\b",
    "otp_like": r"\b\d{4,6}\b"
}

# --- –§–£–ù–ö–¶–ò–ò –ê–ù–ê–õ–ò–ó–ê (–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---

def analyze_sentiment(text: str) -> Dict[str, Any]:
    if not text or tokenizer is None or model is None: 
        return {"sentiment": "AI_ERROR", "score": 0.0}
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probabilities = torch.softmax(outputs.logits, dim=1)[0]
    labels = model.config.id2label
    predicted_index = torch.argmax(probabilities).item()
    sentiment = labels.get(predicted_index, "UNKNOWN")
    score = probabilities[predicted_index].item()
    return {"sentiment": sentiment, "score": round(score, 4)}

def analyze(text: str) -> List[Dict[str, Any]]:
    if not text:
        return []
    low = text.lower()
    findings = []
    
    for phrase in FRAUD_KEYPHRASES:
        if phrase in low:
            findings.append({"type": "keyword (exact)", "phrase": phrase})
            continue
        score_token = fuzz.token_set_ratio(phrase, low)
        if score_token >= FUZZY_THRESHOLD:
            findings.append({"type": "keyword (fuzzy)", "phrase": phrase, "score": round(score_token, 2)})
            continue
        score_partial = fuzz.partial_ratio(phrase, low)
        if score_partial >= FUZZY_THRESHOLD:
            findings.append({"type": "keyword (partial)", "phrase": phrase, "score": round(score_partial, 2)})

    for name, pat in REGEX_PATTERNS.items():
        for m in re.finditer(pat, text):
            value_display = m.group(0)[:50] + "..." if len(m.group(0)) > 50 else m.group(0)
            findings.append({"type": "regex", "pattern": name, "value": value_display})
            
    unique_findings = []
    seen = set()
    for f in findings:
        key = (f["type"], f.get("phrase", ""), f.get("pattern", ""), f.get("value", ""))
        if key not in seen:
            unique_findings.append(f)
            seen.add(key)
            
    return unique_findings

def determine_risk(findings: List[Dict[str, Any]], sentiment_result: Dict[str, Any]) -> str:
    num_findings = len(findings)
    risk_score = 0 
    
    if num_findings > 5:
        risk_score += 15
    elif num_findings > 2:
        risk_score += 10
    elif num_findings > 0:
        risk_score += 5
        
    if sentiment_result.get('sentiment') == 'NEGATIVE' and sentiment_result.get('score', 0) > 0.8:
        risk_score += 7
        
    if risk_score > 20:
        return "–ö–†–ê–ô–ù–ï –í–´–°–û–ö–ò–ô (AI-–û—Ü–µ–Ω–∫–∞)"
    elif risk_score > 12:
        return "–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô"
    elif risk_score > 7:
        return "–í–´–°–û–ö–ò–ô"
    elif risk_score > 0:
        return "–°–†–ï–î–ù–ò–ô"
    else:
        return "–ù–ò–ó–ö–ò–ô"

# --- –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –û–ë–†–ê–ë–û–¢–ö–ò –ê–£–î–ò–û ---

def process_audio_data(audio_data: bytes, file_format: str) -> Dict[str, Any]:
    wav_path = None
    try:
        audio_segment = AudioSegment.from_file(io.BytesIO(audio_data), format=file_format)
        
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
            audio_segment.export(tmpfile.name, format="wav")
            wav_path = tmpfile.name
        
        rec = sr.Recognizer()
        with sr.AudioFile(wav_path) as src:
            audio = rec.record(src)
        
        text = rec.recognize_google(audio, language="ru-RU")
        
        findings = analyze(text)
        sentiment_result = analyze_sentiment(text)
        final_risk = determine_risk(findings, sentiment_result)
        
        return {
            "transcript": text,
            "findings": findings,
            "ai_analysis": sentiment_result,
            "risk_level": final_risk
        }
    except FileNotFoundError:
        return {"error": "–ù–µ –Ω–∞–π–¥–µ–Ω ffmpeg/libav. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏—Ö –∏ –¥–æ–±–∞–≤—å—Ç–µ –≤ PATH."}
    except sr.UnknownValueError:
        return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±–æ–ª–µ–µ —á–µ—Ç–∫–æ–µ –∞—É–¥–∏–æ."}
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: {e}")
        return {"error": f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}"}
    finally:
        if wav_path and os.path.exists(wav_path):
            os.remove(wav_path)

# --- TELEGRAM HANDLERS  ---

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞.\n"
        "–ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å –º–Ω–µ <b>–≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ</b> –∏–ª–∏ <b>–∞—É–¥–∏–æ—Ñ–∞–π–ª</b> (MP3, OGG, M4A) –∏ —è –ø—Ä–æ–≤–µ—Ä—é –µ–≥–æ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ —Å –ø–æ–º–æ—â—å—é AI.",
        parse_mode='HTML' # 
    )

async def handle_audio(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    message = update.message
    
    file_id = None
    file_format = None
    
    if message.voice:
        file_id = message.voice.file_id
        file_format = "ogg"
    elif message.audio:
        file_id = message.audio.file_id
        file_format = message.audio.file_name.split('.')[-1].lower() if message.audio.file_name else "mp3"
    else:
        return

    await message.reply_text("–ü–æ–ª—É—á–∞—é –∏ –Ω–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ. –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –¥–æ –º–∏–Ω—É—Ç—ã...")
    
    try:
        tg_file = await context.bot.get_file(file_id)
        
        with io.BytesIO() as buffer:
            await tg_file.download_to_memory(buffer) 
            file_bytes = buffer.getvalue()

    except Exception as e:
        await message.reply_text(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        logging.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
        return

    result = process_audio_data(file_bytes, file_format)
    
    if "error" in result:
        await message.reply_text(f"–û—à–∏–±–∫–∞: {result['error']}")
        return

    # --- –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ —Å HTML ---
    
    ai_status = f"<b>AI –°–µ–Ω—Ç–∏–º–µ–Ω—Ç:</b> {result['ai_analysis']['sentiment']} (–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['ai_analysis']['score']:.2f})"
    if result['ai_analysis']['sentiment'] == 'AI_ERROR':
         ai_status = f"<b>AI –°–µ–Ω—Ç–∏–º–µ–Ω—Ç:</b> –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏)"


    findings_text = "\n".join([
        f"- {f['type'].upper()}: <code>{escape(f.get('phrase', f.get('pattern')))}</code>"
        for f in result['findings']
    ]) or "‚Äî –ü—Ä–∏–∑–Ω–∞–∫–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã."

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º HTML —Ç–µ–≥–∏ <b>, <i> –∏ <pre> (–¥–ª—è –±–ª–æ–∫–∞ –∫–æ–¥–∞)
    report = (
        f"<b>üìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê</b>\n\n"
        f"<b>–£–†–û–í–ï–ù–¨ –†–ò–°–ö–ê:</b> {result['risk_level']}\n"
        f"<b>–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:</b> {len(result['findings'])}\n"
        f"{ai_status}\n\n"
        f"<b>üö® –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ü—Ä–∏–∑–Ω–∞–∫–∏:</b>\n"
        f"{findings_text}\n\n"
        f"<b>üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç (–Ω–∞—á–∞–ª–æ):</b>\n"
        f"<i>{escape(result['transcript'][:150])}...</i>\n\n" # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–º–∞—Ç—å —Ç–µ–≥–∏
        f"<b>–ü–æ–ª–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç:</b>\n"
        f"<pre>{escape(result['transcript'])}</pre>" 
    )

    await message.reply_text(report, parse_mode='HTML') # 

# --- MAIN BOT RUNNER  ---
def main() -> None:
    if BOT_TOKEN == "–í–ê–®_–¢–û–ö–ï–ù_–ë–û–¢–ê_–ó–î–ï–°–¨":
        logging.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–º–µ–Ω–∏—Ç–µ BOT_TOKEN –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω –≤–∞—à–µ–≥–æ Telegram-–±–æ—Ç–∞.")
        sys.exit(1)
        
    application = Application.builder().token(BOT_TOKEN).build()

    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(MessageHandler(filters.VOICE | filters.AUDIO, handle_audio))

    logging.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π...")
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == "__main__":
    main()